{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:45\n",
      "-------------------------------------------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "lines.pprint()\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(20)  # Wait for the computation to terminate\n",
    "ssc.stop(stopSparkContext=False)  # Stop the StreamingContext without stopping the SparkContext\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:50\n",
      "-------------------------------------------\n",
      "('other', 15486)\n",
      "('first', 10815)\n",
      "('many', 9773)\n",
      "('new', 6272)\n",
      "('system', 5063)\n",
      "('american', 4744)\n",
      "('several', 4545)\n",
      "('century', 4492)\n",
      "('same', 4394)\n",
      "('=', 4313)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:16:55\n",
      "-------------------------------------------\n",
      "('other', 15319)\n",
      "('first', 10709)\n",
      "('many', 9575)\n",
      "('new', 6242)\n",
      "('system', 5111)\n",
      "('american', 4777)\n",
      "('century', 4538)\n",
      "('several', 4515)\n",
      "('same', 4453)\n",
      "('=', 4361)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:17:00\n",
      "-------------------------------------------\n",
      "('other', 15346)\n",
      "('first', 10517)\n",
      "('many', 9706)\n",
      "('new', 6218)\n",
      "('system', 5266)\n",
      "('american', 4940)\n",
      "('several', 4615)\n",
      "('=', 4451)\n",
      "('century', 4438)\n",
      "('same', 4270)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:17:05\n",
      "-------------------------------------------\n",
      "('other', 15196)\n",
      "('first', 10617)\n",
      "('many', 9848)\n",
      "('new', 6289)\n",
      "('system', 5093)\n",
      "('american', 4824)\n",
      "('several', 4519)\n",
      "('century', 4493)\n",
      "('=', 4332)\n",
      "('same', 4260)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-06 12:17:10\n",
      "-------------------------------------------\n",
      "('other', 15091)\n",
      "('first', 10463)\n",
      "('many', 9703)\n",
      "('new', 6175)\n",
      "('system', 5102)\n",
      "('american', 4829)\n",
      "('several', 4523)\n",
      "('century', 4520)\n",
      "('=', 4369)\n",
      "('same', 4325)\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "\n",
    "# split the rdd into 5 equal-size parts\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Do word-counting as before\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use transform() to access any rdd transformations not directly available in SparkStreaming\n",
    "topWords = wordCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "topWords.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(25)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-02 20:09:25\n",
      "-------------------------------------------\n",
      "(7890.0, 'great')\n",
      "(6180.0, 'popular')\n",
      "(5544.0, 'best')\n",
      "(4662.0, 'good')\n",
      "(4242.0, 'important')\n",
      "(2340.0, 'strong')\n",
      "(2322.0, 'greater')\n",
      "(2058.0, 'successful')\n",
      "(1850.0, 'novel')\n",
      "(1790.0, 'natural')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 20:09:30\n",
      "-------------------------------------------\n",
      "(7578.0, 'great')\n",
      "(6126.0, 'popular')\n",
      "(5604.0, 'best')\n",
      "(4749.0, 'good')\n",
      "(4172.0, 'important')\n",
      "(2253.0, 'greater')\n",
      "(2252.0, 'strong')\n",
      "(2001.0, 'successful')\n",
      "(1948.0, 'novel')\n",
      "(1804.0, 'natural')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 20:09:35\n",
      "-------------------------------------------\n",
      "(7893.0, 'great')\n",
      "(6009.0, 'popular')\n",
      "(5574.0, 'best')\n",
      "(4677.0, 'good')\n",
      "(4298.0, 'important')\n",
      "(2326.0, 'strong')\n",
      "(2172.0, 'greater')\n",
      "(1944.0, 'successful')\n",
      "(1868.0, 'novel')\n",
      "(1761.0, 'natural')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 20:09:40\n",
      "-------------------------------------------\n",
      "(7776.0, 'great')\n",
      "(5925.0, 'popular')\n",
      "(5538.0, 'best')\n",
      "(4671.0, 'good')\n",
      "(4156.0, 'important')\n",
      "(2362.0, 'strong')\n",
      "(2205.0, 'greater')\n",
      "(1932.0, 'successful')\n",
      "(1892.0, 'novel')\n",
      "(1803.0, 'natural')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 20:09:45\n",
      "-------------------------------------------\n",
      "(7566.0, 'great')\n",
      "(5916.0, 'popular')\n",
      "(5334.0, 'best')\n",
      "(4548.0, 'good')\n",
      "(4268.0, 'important')\n",
      "(2415.0, 'greater')\n",
      "(2314.0, 'strong')\n",
      "(2109.0, 'successful')\n",
      "(2018.0, 'novel')\n",
      "(1821.0, 'natural')\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Find the most positive words in windows of 5 seconds from streaming data\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = sc.textFile(\"../data/AFINN-111.txt\") \\\n",
    "                    .map(parse_line).cache()\n",
    "    \n",
    "ssc = StreamingContext(sc, 5)\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Determine the words with the highest sentiment values by joining the streaming RDD\n",
    "# with the static RDD inside the transform() method and then multiplying\n",
    "# the frequency of the words by its sentiment value\n",
    "happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n",
    "                            .map(lambda t:\n",
    "                                 (t[1][0] * t[1][1], t[0])) \\\n",
    "                            .transform(lambda rdd: rdd.sortByKey(False))\n",
    "\n",
    "happiest_words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(25)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  51430\n",
      "[('other', 7782), ('first', 5404), ('many', 4878), ('new', 3219), ('system', 2539)]\n",
      "refinery: 1\n",
      "Total distinct words:  76917\n",
      "[('other', 15486), ('first', 10815), ('many', 9773), ('new', 6272), ('system', 5063)]\n",
      "refinery: 6\n",
      "Total distinct words:  97338\n",
      "[('other', 23129), ('first', 16145), ('many', 14534), ('new', 9363), ('system', 7636)]\n",
      "refinery: 11\n",
      "Total distinct words:  114786\n",
      "[('other', 30805), ('first', 21524), ('many', 19348), ('new', 12514), ('system', 10174)]\n",
      "refinery: 16\n",
      "Total distinct words:  130393\n",
      "[('other', 38452), ('first', 26792), ('many', 24239), ('new', 15618), ('system', 12777)]\n",
      "refinery: 19\n",
      "Total distinct words:  144898\n",
      "[('other', 46151), ('first', 32041), ('many', 29054), ('new', 18732), ('system', 15440)]\n",
      "refinery: 23\n",
      "Total distinct words:  158013\n",
      "[('other', 53728), ('first', 37395), ('many', 33933), ('new', 21787), ('system', 17946)]\n",
      "refinery: 26\n",
      "Total distinct words:  170610\n",
      "[('other', 61347), ('first', 42658), ('many', 38902), ('new', 25021), ('system', 20533)]\n",
      "refinery: 29\n",
      "Total distinct words:  182330\n",
      "[('other', 68982), ('first', 47856), ('many', 43780), ('new', 28095), ('system', 23031)]\n",
      "refinery: 37\n",
      "Total distinct words:  193450\n",
      "[('other', 76438), ('first', 53121), ('many', 48605), ('new', 31196), ('system', 25635)]\n",
      "refinery: 40\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Stateful word count\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.take(5))\n",
    "    print('refinery:', rdd.lookup('refinery')[0])\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  51430\n",
      "[('other', 7782, 7782), ('first', 5404, 5404), ('many', 4878, 4878), ('new', 3219, 3219), ('system', 2539, 2539)]\n",
      "refinery: 1 , 1\n",
      "Next threshold =  5\n",
      "Total distinct words:  13971\n",
      "[('other', 15481, 15486), ('first', 10810, 10815), ('many', 9768, 9773), ('new', 6267, 6272), ('system', 5058, 5063)]\n",
      "refinery: 1 , 6\n",
      "Next threshold =  5\n",
      "Total distinct words:  12164\n",
      "[('other', 23119, 23129), ('first', 16135, 16145), ('many', 14524, 14534), ('new', 9353, 9363), ('system', 7626, 7636)]\n",
      "refinery: 1 , 11\n",
      "Next threshold =  4\n",
      "Total distinct words:  12317\n",
      "[('other', 30791, 30805), ('first', 21510, 21524), ('many', 19334, 19348), ('new', 12500, 12514), ('system', 10160, 10174)]\n",
      "refinery: 2 , 16\n",
      "Next threshold =  5\n",
      "Total distinct words:  11650\n",
      "[('other', 38433, 38452), ('first', 26773, 26792), ('many', 24220, 24239), ('new', 15599, 15618), ('system', 12758, 12777)]\n",
      "refinery: 0 , 19\n",
      "Next threshold =  5\n",
      "Total distinct words:  11396\n",
      "[('other', 46127, 46151), ('first', 32017, 32041), ('many', 29030, 29054), ('new', 18708, 18732), ('system', 15416, 15440)]\n",
      "refinery: 0 , 24\n",
      "Next threshold =  4\n",
      "Total distinct words:  11963\n",
      "[('other', 53700, 53728), ('first', 37367, 37395), ('many', 33905, 33933), ('new', 21759, 21787), ('system', 17918, 17946)]\n",
      "refinery: 0 , 28\n",
      "Next threshold =  5\n",
      "Total distinct words:  11415\n",
      "[('other', 61314, 61347), ('first', 42625, 42658), ('many', 38869, 38902), ('new', 24988, 25021), ('system', 20500, 20533)]\n",
      "refinery: 0 , 33\n",
      "Next threshold =  5\n",
      "Total distinct words:  11314\n",
      "[('other', 68944, 68982), ('first', 47818, 47856), ('many', 43742, 43780), ('new', 28057, 28095), ('system', 22993, 23031)]\n",
      "refinery: 3 , 41\n",
      "Next threshold =  5\n",
      "Total distinct words:  11204\n",
      "[('other', 76395, 76438), ('first', 53078, 53121), ('many', 48562, 48605), ('new', 31153, 31196), ('system', 25592, 25635)]\n",
      "refinery: 1 , 44\n",
      "Next threshold =  5\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# MG algorithm for approximate word count\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "k = 10000\n",
    "threshold = 0\n",
    "total_decrement = 0\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    newValue = sum(newValues, runningCount) - threshold\n",
    "    return newValue if newValue > 0 else None\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "            \n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    global threshold, total_decrement \n",
    "    rdd.cache()\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.map(lambda x: (x[0], x[1], x[1]+total_decrement)).take(5))\n",
    "    lower_bound = rdd.lookup('refinery')\n",
    "    if len(lower_bound) > 0:\n",
    "        lower_bound = lower_bound[0]\n",
    "    else:\n",
    "        lower_bound = 0\n",
    "    print('refinery:', lower_bound, ',', lower_bound + total_decrement)\n",
    "    if rdd.count() > k:\n",
    "        threshold = rdd.zipWithIndex().map(lambda x: (x[1], x[0])).lookup(k)[0][1]\n",
    "    else:\n",
    "        threhold = 0\n",
    "    print(\"Next threshold = \", threshold)\n",
    "    total_decrement += threshold\n",
    "    rdd.unpersist()\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:00:48\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:00:51\n",
      "-------------------------------------------\n",
      "5\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:00:54\n",
      "-------------------------------------------\n",
      "15\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:00:57\n",
      "-------------------------------------------\n",
      "30\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:01:00\n",
      "-------------------------------------------\n",
      "45\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:01:03\n",
      "-------------------------------------------\n",
      "35\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:01:06\n",
      "-------------------------------------------\n",
      "20\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-02 22:01:09\n",
      "-------------------------------------------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    rdd = sc.parallelize([i, i, i, i, i])\n",
    "    rddQueue += [rdd]\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 3 seconds\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "nums = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Compute the sum over a sliding window of 9 seconds for every 3 seconds\n",
    "# slidingSum = nums.reduceByWindow(lambda x, y: x + y, None, 9, 3)\n",
    "slidingSum = nums.reduceByWindow(lambda x, y: x + y, lambda x, y: x - y, 9, 3)\n",
    "\n",
    "slidingSum.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(24)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Word count using structured streaming: Complete mode vs update mode\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "word_counts = words.groupBy('word').count()\n",
    "\n",
    "# Start running the query \n",
    "query = word_counts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Append mode with selection condition\n",
    "# Note: complete mode not supported if no aggregation\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "long_words = words.filter(length(words['word'])>=3)\n",
    "\n",
    "# Start running the query \n",
    "query = long_words\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 seconds\", \"5 seconds\"),\n",
    "    words.word)\\\n",
    "    .count()\n",
    "\n",
    "# Start running the query \n",
    "query = windowedCounts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
